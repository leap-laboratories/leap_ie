{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s4RHeATLTkta",
    "outputId": "f325f3a4-7c17-4e02-a4c4-6e1de10ba486"
   },
   "outputs": [],
   "source": [
    "!pip install leap-ie --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8BGSbidO37G",
    "outputId": "a755d2f3-5e15-4ec7-ae16-4140108b568c"
   },
   "outputs": [],
   "source": [
    "from leap_ie.vision import engine\n",
    "from leap_ie.vision.models import get_model\n",
    "\n",
    "preprocessing_fn, model, class_list = get_model('torchvision.resnet18')\n",
    "\n",
    "config = {\"leap_api_key\": \"YOUR_API_KEY\"}\n",
    "\n",
    "results_df, results_dict = engine.generate(project_name=\"leap_demo\", model=model, class_list=class_list, config = config, target_classes=[1], preprocessing=preprocessing_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "OAscWcEa-T1H",
    "outputId": "14518eed-e7ca-484b-b337-f55c0a830bf9"
   },
   "outputs": [],
   "source": [
    "from leap_ie.vision import engine\n",
    "from leap_ie.vision.models import get_model\n",
    "\n",
    "preprocessing_fn, model, class_list = get_model('torchvision.resnet18')\n",
    "\n",
    "config = {\"wandb_api_key\": \"YOUR_WANDB_API_KEY\",\n",
    "          \"wandb_entity\": \"leap-labs\",\n",
    "          \"leap_api_key\": \"YOUR_API_KEY\"\n",
    "          \"isolation\":False}\n",
    "\n",
    "results_df, results_dict = engine.generate(project_name=\"leap_demo\", model=model, class_list=class_list, config = config, target_classes=[292, 483, 951], preprocessing=preprocessing_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "xN25fvDX_Cfo",
    "outputId": "7c18aaf0-53fb-4a34-d609-f25616249f67"
   },
   "outputs": [],
   "source": [
    "from leap_ie.vision import engine\n",
    "from leap_ie.vision.models import get_model\n",
    "\n",
    "preprocessing_fn, model, class_list = get_model('torchvision.resnet101')\n",
    "\n",
    "config = {\"wandb_api_key\": \"YOUR_WANDB_API_KEY\",\n",
    "          \"wandb_entity\": \"leap-labs\",\n",
    "          \"leap_api_key\": \"YOUR_API_KEY\",\n",
    "          \"isolation\":False}\n",
    "\n",
    "results_df, results_dict = engine.generate(project_name=\"leap_demo\", model=model, class_list=class_list, config = config, target_classes=[292, 483, 951], preprocessing=preprocessing_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "QegyisUvASXk",
    "outputId": "e3cb55d1-e94d-4744-a475-9b9a4aa253fc"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "preprocessing_fn, model, class_list = get_model('torchvision.resnet101')\n",
    "\n",
    "config = {\"wandb_api_key\": \"YOUR_WANDB_API_KEY\",\n",
    "          \"wandb_entity\": \"leap-labs\",\n",
    "          \"leap_api_key\": \"YOUR_API_KEY\"}\n",
    "\n",
    "def load_image(url):\n",
    "    # Get image from URL\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "    # Define transformations\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(224, interpolation=Image.BICUBIC),  # Resize the image so that the maximum dimension is 224\n",
    "        transforms.CenterCrop(224),  # Center crop the image to 224x224\n",
    "        transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    ])\n",
    "\n",
    "    # Apply transformations\n",
    "    input_tensor = preprocess(img)\n",
    "\n",
    "    # Create a mini-batch as expected by the model\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "    return input_batch\n",
    "\n",
    "url = \"https://limitedabode.co.uk/cdn/shop/products/fruit-bowl-min_1800x1800.jpg?v=1643205800\"\n",
    "input_tensor = load_image(url)\n",
    "\n",
    "results_df, results_dict = engine.generate(project_name=\"leap_demo\", model=model, class_list=class_list, config = config, preprocessing=preprocessing_fn, samples=input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5314wyzA6q7c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
